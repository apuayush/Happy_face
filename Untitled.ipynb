{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train = h5py.File('dataset/train_happy.h5', \"r\")\n",
    "    X_train_orig = np.array(train[\"train_set_x\"][:]) # your train set features\n",
    "    Y_train_orig = np.array(train[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('dataset/test_happy.h5', \"r\")\n",
    "    X_test_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    Y_test_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    Y_train = Y_train_orig.reshape((1, Y_train_orig.shape[0]))\n",
    "    Y_test = Y_test_orig.reshape((1, Y_test_orig.shape[0]))\n",
    "    \n",
    "    return X_train_orig, Y_train, X_test_orig, Y_test, classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_o, Y_train_o, X_test_o, Y_test_o, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 1) (600, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_o/255\n",
    "X_test = X_test_o/255\n",
    "\n",
    "Y_train = Y_train_o.T\n",
    "Y_test = Y_test_o.T\n",
    "\n",
    "print(Y_train.shape, X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HappyModel(input_shape):\n",
    "   \n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((2, 2), name='max_pool')(X)\n",
    "\n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X)\n",
    "    model = Model(inputs = X_input, outputs = X, name='HappyModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HappyModel((64,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 2.6069 - acc: 0.5717\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.7397 - acc: 0.7400\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.2441 - acc: 0.9050\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.1484 - acc: 0.9450\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.1187 - acc: 0.9667\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.0938 - acc: 0.9750\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.1006 - acc: 0.9650\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.0775 - acc: 0.9783\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0683 - acc: 0.9817\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0650 - acc: 0.9833\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.1028 - acc: 0.9683\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0779 - acc: 0.9717\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.1130 - acc: 0.9550\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 0.0774 - acc: 0.9650\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 0.0502 - acc: 0.9883\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0509 - acc: 0.9850\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 0.0343 - acc: 0.9933\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 0.0364 - acc: 0.9950\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 6s 10ms/step - loss: 0.0426 - acc: 0.9900\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 5s 9ms/step - loss: 0.0735 - acc: 0.9700\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0637 - acc: 0.9783\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0552 - acc: 0.9800\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0352 - acc: 0.9867\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0337 - acc: 0.9900\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0177 - acc: 0.9950\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0275 - acc: 0.9950\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0340 - acc: 0.9900\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0566 - acc: 0.9750\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0273 - acc: 0.9950\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0437 - acc: 0.9850\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0231 - acc: 0.9950\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0240 - acc: 0.9950\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0199 - acc: 0.9933\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 6s 9ms/step - loss: 0.0125 - acc: 0.9967\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0136 - acc: 0.9933\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0076 - acc: 0.9967\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0108 - acc: 0.9983\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0174 - acc: 0.9933\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0162 - acc: 0.9950\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0318 - acc: 0.9900\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0140 - acc: 0.9933\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0106 - acc: 0.9983\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0128 - acc: 0.9983\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0098 - acc: 0.9983\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0151 - acc: 0.9950\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0154 - acc: 0.9933\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0072 - acc: 0.9967\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0075 - acc: 0.9983\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0163 - acc: 0.9950\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0137 - acc: 0.9967\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0135 - acc: 0.9950\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0122 - acc: 0.9967\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0098 - acc: 0.9983\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0090 - acc: 0.9967\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0177 - acc: 0.9933\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.1570 - acc: 0.9400\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.3472 - acc: 0.9083\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.1991 - acc: 0.9283\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.1135 - acc: 0.9583\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0360 - acc: 0.9850\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0645 - acc: 0.9800\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0437 - acc: 0.9817\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0830 - acc: 0.9700\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.1260 - acc: 0.9667\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0300 - acc: 0.9900\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0139 - acc: 0.9933\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0351 - acc: 0.9900\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0510 - acc: 0.9817\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0117 - acc: 0.9950\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0369 - acc: 0.9867\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0393 - acc: 0.9850\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0147 - acc: 0.9933\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0080 - acc: 0.9983\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0177 - acc: 0.9917\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0244 - acc: 0.9917\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0140 - acc: 0.9983\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0068 - acc: 0.9983\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0046 - acc: 0.9983\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0088 - acc: 0.9950\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0286 - acc: 0.9917\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0116 - acc: 0.9933\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0351 - acc: 0.9917\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0823 - acc: 0.9667\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0180 - acc: 0.9917\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0054 - acc: 0.9950\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0166 - acc: 0.9950\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0054 - acc: 0.9983\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0112 - acc: 0.9967\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0064 - acc: 0.9983\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.0035 - acc: 0.9967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3a7c39c780>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=Y_train, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 1s 4ms/step\n",
      "\n",
      "Loss = 0.03869444911678632\n",
      "Test Accuracy = 98.66666642824808 %\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(x=X_test, y=Y_test)\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]*100) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
